---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---



<span class='anchor' id='about-me'></span>

Hi, I am Yuankun Xie, a fourth-year Ph.D. candidate at the Communication University of China, 
and a joint Ph.D. student at the Institute of Automation, Chinese Academy of Sciences.

My research interests include **audio deepfake detection** (global detection, partial deepfake localization, and source tracing), 
**audio large language models (ALLMs)**, domain generalization, out-of-distribution detection, and neural audio codecs. 
I have published <span style="color: red; font-weight: bold;">20+</span> papers in top-tier international AI conferences and journals, 
including **TIFS, AAAI, IJCAI, ICASSP, and INTERSPEECH**.


# üî• Milestones
- *2026.01*: &nbsp; üéâ 2 papers accepted by <span style="color: red; font-weight: bold;">ICASSP 2026</span>.
- *2025.11*: &nbsp; üéâ 1 papers accepted by <span style="color: red; font-weight: bold;">AAAI 2026</span>.
- *2025.11*: &nbsp; üíª Ranked <span style="color: red; font-weight: bold;">1st place</span> in both Track 1 and Track 2 of the ESDD Competition.
- *2025.07*: &nbsp; üíª Ranked <span style="color: red; font-weight: bold;">3rd</span> in Track 3 of the Alibaba Tianchi 2025 Global AI Attack and Defense Challenge.
- *2024.12*: &nbsp; üéâ 1 journal papers accepted by <span style="color: red; font-weight: bold;">TASLP</span>.
- *2024.09*: &nbsp; üåè Attended INTERSPEECH 2024 in Greece, presenting one poster and one oral presentation.
- *2024.08*: &nbsp; üéâ 1 papers accepted by <span style="color: red; font-weight: bold;">INTERSPEECH 2024W (ASVspoof2024)</span>.
- *2024.08*: &nbsp; üéâ 3 papers accepted by <span style="color: red; font-weight: bold;">ISCSLP 2024</span>.
- *2024.06*: &nbsp; üéâ 4 papers accepted by <span style="color: red; font-weight: bold;">INTERSPEECH 2024</span>.
- *2024.04*: &nbsp; üåè Attended ICASSP 2024 in Korea, presenting one poster and one oral presentation.
- *2024.01*: &nbsp; üéâ 2 papers accepted by <span style="color: red; font-weight: bold;">ICASSP 2024</span>.
- *2023.11*: &nbsp; üëì Joined Professor [Jianhua Tao](https://scholar.google.com/citations?user=781jbHMAAAAJ&hl=zh-CN)'s group at the Institute of Automation for my Ph.D. joint training, under the specific guidance of Dr. [Ruibo Fu](https://scholar.google.com/citations?user=xrTAz_gAAAAJ&hl=zh-CN).
- *2023.10*: &nbsp; üéâ 1 journal paper accepted by <span style="color: red; font-weight: bold;">TIFS</span>.
- *2023.08*: &nbsp; üåè Attended IJCAI 2023 DADA workshop (ADD2023) in Macao, delivering one presentation.
- *2023.06*: &nbsp; üéâ 2 papers accepted by <span style="color: red; font-weight: bold;">INTERSPEECH 2023</span> and <span style="color: red; font-weight: bold;">IJCAI 2023 DADA workshop</span>.
- *2023.05*: &nbsp; üíª Ranked <span style="color: red; font-weight: bold;">6 / 14</span> in Track 1.1, <span style="color: red; font-weight: bold;">5 / 52</span> in Track 1.2, and <span style="color: red; font-weight: bold;">6 / 17</span> in Track 2 of the ADD2023 Competition.
- *2022.09*: &nbsp; üëì Joined Professor [Long Ye](https://ices.cuc.edu.cn/2020/0821/c5332a172456/pagem.htm)'s group at the Communication University of China, under the specific guidance of Dr. [Haonan Cheng](https://haonancheng.cn/).


# üìù First-Author Publications 
<span class='anchor' id='publications'></span>
To learn more about my publications, please visit my [Google Scholar page](https://scholar.google.com/citations?user=1KAs5egAAAAJ&hl=zh-CN&oi=ao). 
### Journal
- **J2-TASLP 2025:** The Codecfake Dataset and Countermeasures for the Universally Detection of Deepfake Audio [[paper]](https://ieeexplore.ieee.org/abstract/document/10830534/)[[code]](https://github.com/xieyuankun/Codecfake)

  <span style="color: red; font-weight: bold;">Yuankun Xie</span>, Yi Lu, Ruibo Fu, Zhengqi Wen, Zhiyong Wang, Jianhua Tao, Xin Qi, Xiaopeng Wang, Yukun Liu, Haonan Cheng, Long Ye, Yi Sun
  
- **J1-TIFS 2024:** Domain Generalization Via Aggregation and Separation for Audio Deepfake Detection [[paper]](https://ieeexplore.ieee.org/abstract/document/10286049/)

  <span style="color: red; font-weight: bold;">Yuankun Xie</span>, Haonan Cheng, Yutian Wang, Long Ye



### Conference

- **C11-AAAI 2026 :** Detect All-Type Deepfake Audio: Wavelet Prompt Tuning for Enhanced Auditory Perception [[paper]](https://arxiv.org/pdf/2504.06753)[[code]](https://github.com/xieyuankun/All-Type-ADD)
  
  <span style="color: red; font-weight: bold;">Yuankun Xie</span>, Ruibo Fu, Zhiyong Wang, Xiaopeng Wang, Songjun Cao, Long Ma, Haonan Cheng, Long Ye

<!-- - **C10-ICASSP 2026 (submitted):** Neural Codec Source Tracing: Toward Comprehensive Attribution in Open-Set Condition [[paper]](https://arxiv.org/abs/2501.06514)[[code]](https://github.com/xieyuankun/ST-Codecfake)
  
  <span style="color: red; font-weight: bold;">Yuankun Xie</span>, Xiaopeng Wang, Zhiyong Wang, Ruibo Fu, Zhengqi Wen, Songjun Cao, Long Ma, Chenxing Li, Haonnan Cheng, Long Ye -->
- **C10-ICASSP 2026 :** Fake Speech Wild: Detecting Deepfake Speech on Social Media Platform [[paper]](https://arxiv.org/abs/2508.10559)
  
  <span style="color: red; font-weight: bold;">Yuankun Xie</span>, Ruibo Fu, Xiaopeng Wang, Zhiyong Wang, Ya Li, Zhengqi Wen, Haonnan Cheng, Long Ye
  
- **C9-INTERSPEECH 2024W (ASVspoof2024):** Temporal Variability and Multi-Viewed Self-Supervised Representations to Tackle the ASVspoof5 Deepfake Challenge [[paper]](https://arxiv.org/abs/2408.06922)
  
  <span style="color: red; font-weight: bold;">Yuankun Xie</span>, Xiaopeng Wang, Zhiyong Wang, Ruibo Fu, Zhengqi Wen, Haonan Cheng, Long Ye

- **C8-ISCSLP 2024:** Does Current Deepfake Audio Detection Model Effectively Detect ALM-based Deepfake Audio? [[paper]](https://ieeexplore.ieee.org/abstract/document/10800375)[[code]](https://github.com/xieyuankun/ALM-ADD)
  
  <span style="color: red; font-weight: bold;">Yuankun Xie</span>, Chenxu Xiong, Xiaopeng Wang, Zhiyong Wang, Yi Lu, Xin Qi, Ruibo Fu, Yukun Liu, Zhengqi Wen, Jianhua Tao, et al.

- **C7-INTERSPEECH 2024:** Generalized Source Tracing: Detecting Novel Audio Deepfake Algorithm with Real Emphasis and Fake Dispersion strategy [[paper]](https://arxiv.org/abs/2406.03240)[[code]](https://github.com/xieyuankun/REFD)
  
  <span style="color: red; font-weight: bold;">Yuankun Xie</span>, Ruibo Fu, Zhengqi Wen, Zhiyong Wang, Xiaopeng Wang, Haonnan Cheng, Long Ye, Jianhua Tao

- **C6-INTERSPEECH 2024:** Codecfake: An Initial Dataset for Detecting LLM-based Deepfake Audio [[paper]](https://arxiv.org/abs/2406.08112)[[code]](https://github.com/xieyuankun/Codecfake)
  
  Yi Lu‚Ä†, <span style="color: red; font-weight: bold;">Yuankun Xie‚Ä†</span>, Ruibo Fu, Zhengqi Wen, Jianhua Tao, Zhiyong Wang, Xin Qi,
  Xuefei Liu, Yongwei Li, Yukun Liu, et al.


- **C5-ICASSP 2024:** An Efficient Temporary Deepfake Location Approach Based Embeddings for Partially Spoofed Audio Detection [[paper]](https://ieeexplore.ieee.org/abstract/document/10448196)[[code]](https://github.com/xieyuankun/TDL-ADD)
  
  <span style="color: red; font-weight: bold;">Yuankun Xie</span>, Haonan Cheng, Yutian Wang, Long Ye


- **C4-ICASSP 2024:** FSD: An initial chinese dataset for fake song detection [[paper]](https://ieeexplore.ieee.org/abstract/document/10446271/)[[code]](https://github.com/xieyuankun/FSD-Dataset)
  
  <span style="color: red; font-weight: bold;">Yuankun Xie</span>, Jingjing Zhou, Xiaolin Lu, Zhenghao Jiang, Yuxin Yang, Haonan Cheng, Long Ye


- **C3-IJCAI 2023:** Single domain generalization for audio deepfake detection [[paper]](http://addchallenge.cn/files/2023/pdf/p58-xie.pdf)
  
  <span style="color: red; font-weight: bold;">Yuankun Xie</span>, Haonan Cheng, Yutian Wang, Long Ye


- **C2-INTERSPEECH 2023:** Learning A Self-Supervised Domain-Invariant Feature Representation for Generalized Audio Deepfake Detection [[paper]](https://www.isca-archive.org/interspeech_2023/xie23c_interspeech.pdf)
  
   <span style="color: red; font-weight: bold;">Yuankun Xie</span>, Haonan Cheng, Yutian Wang, Long Ye

- **C1-ICME 2023:** Unsupervised quantized prosody representation for controllable speech synthesis [[paper]](https://ieeexplore.ieee.org/abstract/document/9859946)
  
  Yutian Wang‚Ä†,  <span style="color: red; font-weight: bold;">Yuankun Xie‚Ä†</span>, Kun Zhao, Hui Wang, Qin Zhang

# üè¢ Industrial Experiences
<span class='anchor' id='industrial'></span>

- *2024.11 - 2025.4* Research Intern, **Tencent YouTu Lab** (Beijing, China)
  + (ICASSP26) Focused on audio deepfake detection on social media platform.
  + (Neurocomputing) Open-set neural codec source tracing and explainable ALM-based deepfake audio detection.
  + (AAAI26) Developed universally cross-type audio deepfake coutermeasure (including speech, sound, singing voice, and music).

- *2025.6 - 2025.8* Research Intern, **ByteDance** (Beijing, China)

  + Focused on the task of synthesizing first-order ambisonic spatial audio from 360-degree video and spatial audio captions.

- *2025.10 - now* Research Intern, **Ant Group** (Beijing, China)

  + (ACL26 submitted) Focused on Interpretable All-Type Audio Deepfake Detection with Audio LLMs.

# üíª Competition
- *2025.11* ICASSP 2026 [Environmental Sound Deepfake Detection Challenge](https://www.codabench.org/competitions/10014/#/results-tab), Track1 1/24, Track2 1/15
- *2025.7* Alibaba-Tianchi [2025 Global AI Attack and Defense Challenge Track 3](https://tianchi.aliyun.com/competition/entrance/532394), preliminary round 1/60, final round rank 3/37.
- *2024.7* IJCAI 2024 [The 9th FinVolution Global Data Science Competition: Deepfake Speech Detection Challenge](https://ai.ppdai.com/mirror/goToMirrorDetailSix?mirrorId=34&tabindex=2), preliminary round 2/202, final round 12/30.
- *2023.5* IJCAI 2023 DADA workshop Track 1.1, 6/14 
- *2023.5* IJCAI 2023 [DADA workshop Track 1.2](https://codalab.lisn.upsaclay.fr/competitions/11359#results), 5/52
- *2023.5* IJCAI 2023 [DADA workshop Track 2](https://codalab.lisn.upsaclay.fr/competitions/11361#results), 6/17 


# üåè Visitor Map
<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=600&t=tt&d=Ll_923J16YZ79I-H6twDqzPNsFpAv-lzwzFg-BGT8tI&co=2d78ad&cmo=3acc3a&cmn=ff5353&ct=ffffff'></script>
